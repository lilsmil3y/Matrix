{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilsmil3y/Matrix/blob/main/so_vits_svc_4_0_inference_FIXED_thanks_goat_YeezyBeaver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "2P9H2ubS5msc",
        "outputId": "4254d478-3388-477e-df83-dfb7b6230cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 23 22:20:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-4eVF_7T_YF",
        "cellView": "form",
        "outputId": "fe40d960-5a28-4328-bcde-4ce7d9725a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'so-vits-svc'...\n",
            "remote: Enumerating objects: 1244, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1244 (delta 19), reused 35 (delta 17), pack-reused 1202\u001b[K\n",
            "Receiving objects: 100% (1244/1244), 24.28 MiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (691/691), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.9/dist-packages (2.2.3)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from Flask) (8.1.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from Flask) (2.2.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from Flask) (6.4.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from Flask) (3.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->Flask) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->Flask) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting Flask_Cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.9/dist-packages (from Flask_Cors) (1.16.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.9/dist-packages (from Flask_Cors) (2.2.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=0.9->Flask_Cors) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=0.9->Flask_Cors) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=0.9->Flask_Cors) (6.4.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from Flask>=0.9->Flask_Cors) (2.2.3)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=0.9->Flask_Cors) (8.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->Flask>=0.9->Flask_Cors) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->Flask>=0.9->Flask_Cors) (2.1.2)\n",
            "Installing collected packages: Flask_Cors\n",
            "Successfully installed Flask_Cors-3.0.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.27.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-11.0.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Collecting gradio-client>=0.1.3\n",
            "  Downloading gradio_client-0.1.3-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.7)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.8.10-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.5.3)\n",
            "Collecting huggingface-hub>=0.13.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=5a31f99454438140885e4b5d3471cdf9cd7648499fb50e5dee7581b44fc1e154\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, multidict, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, aiosignal, httpx, fastapi, aiohttp, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 fastapi-0.95.1 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.27.0 gradio-client-0.1.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.13.4 linkify-it-py-2.0.0 mdit-py-plugins-0.3.3 multidict-6.0.4 orjson-3.8.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0.2 yarl-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "Successfully installed numpy-1.23.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.9/dist-packages (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba==0.56.4) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba==0.56.4) (67.6.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.9/dist-packages (from numba==0.56.4) (1.23.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting pyworld==0.3.0\n",
            "  Downloading pyworld-0.3.0.tar.gz (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.0/212.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from pyworld==0.3.0) (0.29.34)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pyworld==0.3.0) (1.23.5)\n",
            "Building wheels for collected packages: pyworld\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.0-cp39-cp39-linux_x86_64.whl size=892323 sha256=08f348acd17ed03819b41c62701e482d99ea1df33a656a01ec192b6fd7fea715\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/84/0e/28419b78625f2aebeba7f4e2714c114da8db2db7527dd166a9\n",
            "Successfully built pyworld\n",
            "Installing collected packages: pyworld\n",
            "Successfully installed pyworld-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy==1.10.1) (1.23.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: SoundFile==0.12.1 in /usr/local/lib/python3.9/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from SoundFile==0.12.1) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->SoundFile==0.12.1) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl (1977.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m901.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchaudio==0.13.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->torchaudio==0.13.1) (4.5.0)\n",
            "Installing collected packages: torchaudio\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "Successfully installed torchaudio-0.13.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting scikit-maad\n",
            "  Downloading scikit_maad-1.3.12-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-maad) (1.5.3)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.9/dist-packages (from scikit-maad) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from scikit-maad) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from scikit-maad) (1.10.1)\n",
            "Collecting resampy>=0.2\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1->scikit-maad) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1->scikit-maad) (2.8.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.9/dist-packages (from resampy>=0.2->scikit-maad) (0.56.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (2.25.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (23.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.17->scikit-maad) (2023.4.12)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.53->resampy>=0.2->scikit-maad) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.53->resampy>=0.2->scikit-maad) (67.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1->scikit-maad) (1.16.0)\n",
            "Installing collected packages: resampy, scikit-maad\n",
            "Successfully installed resampy-0.4.2 scikit-maad-1.3.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from praat-parselmouth) (1.23.5)\n",
            "Installing collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.24-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.9/dist-packages (from onnxsim) (13.3.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.9/dist-packages (from onnxsim) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxsim) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxsim) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxsim) (4.5.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich->onnxsim) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich->onnxsim) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Installing collected packages: onnxsim\n",
            "Successfully installed onnxsim-0.4.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting onnxoptimizer\n",
            "  Downloading onnxoptimizer-0.3.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (678 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m678.2/678.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.9/dist-packages (from onnxoptimizer) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (4.5.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (3.20.3)\n",
            "Installing collected packages: onnxoptimizer\n",
            "Successfully installed onnxoptimizer-0.3.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting fairseq==0.12.2\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Setup 1 (just run this once)\n",
        "import os\n",
        "import glob\n",
        "!git clone https://github.com/effusiveperiscope/so-vits-svc -b eff-4.0\n",
        "os.chdir('so-vits-svc')\n",
        "# install requirements one-at-a-time to ignore exceptions\n",
        "!cat requirements.txt | xargs -n 1 pip install --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install praat-parselmouth\n",
        "!pip install ipywidgets\n",
        "!pip install huggingface_hub\n",
        "!pip install pip==23.0.1 # fix pip version for fairseq install\n",
        "!pip install fairseq==0.12.2\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "existing_files = glob.glob('/content/**/*.*', recursive=True)\n",
        "!pip install numpy==1.21\n",
        "!pip install --upgrade protobuf=3.9.2\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==2.11.0 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup 2 (just run this once)\n",
        "os.chdir('/content/so-vits-svc') # force working-directory to so-vits-svc - this line is just for safety and is probably not required\n",
        "\n",
        "import tarfile\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "# taken from https://github.com/CookiePPP/cookietts/blob/master/CookieTTS/utils/dataset/extract_unknown.py\n",
        "def extract(path):\n",
        "    if path.endswith(\".zip\"):\n",
        "        with ZipFile(path, 'r') as zipObj:\n",
        "           zipObj.extractall(os.path.split(path)[0])\n",
        "    elif path.endswith(\".tar.bz2\"):\n",
        "        tar = tarfile.open(path, \"r:bz2\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".tar.gz\"):\n",
        "        tar = tarfile.open(path, \"r:gz\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".tar\"):\n",
        "        tar = tarfile.open(path, \"r:\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".7z\"):\n",
        "        import py7zr\n",
        "        archive = py7zr.SevenZipFile(path, mode='r')\n",
        "        archive.extractall(path=os.path.split(path)[0])\n",
        "        archive.close()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{path} extension not implemented.\")\n",
        "\n",
        "# taken from https://github.com/CookiePPP/cookietts/tree/master/CookieTTS/_0_download/scripts\n",
        "\n",
        "# megatools download urls\n",
        "win64_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-win64.zip\"\n",
        "win32_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-win32.zip\"\n",
        "linux_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-linux-x86_64.tar.gz\"\n",
        "# download megatools\n",
        "from sys import platform\n",
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "from time import sleep\n",
        "\n",
        "if platform == \"linux\" or platform == \"linux2\":\n",
        "        dl_url = linux_url\n",
        "elif platform == \"darwin\":\n",
        "    raise NotImplementedError('MacOS not supported.')\n",
        "elif platform == \"win32\":\n",
        "        dl_url = win64_url\n",
        "else:\n",
        "    raise NotImplementedError ('Unknown Operating System.')\n",
        "\n",
        "dlname = dl_url.split(\"/\")[-1]\n",
        "if dlname.endswith(\".zip\"):\n",
        "    binary_folder = dlname[:-4] # remove .zip\n",
        "elif dlname.endswith(\".tar.gz\"):\n",
        "    binary_folder = dlname[:-7] # remove .tar.gz\n",
        "else:\n",
        "    raise NameError('downloaded megatools has unknown archive file extension!')\n",
        "\n",
        "if not os.path.exists(binary_folder):\n",
        "    print('\"megatools\" not found. Downloading...')\n",
        "    if not os.path.exists(dlname):\n",
        "        urllib.request.urlretrieve(dl_url, dlname)\n",
        "    assert os.path.exists(dlname), 'failed to download.'\n",
        "    extract(dlname)\n",
        "    sleep(0.10)\n",
        "    os.unlink(dlname)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "binary_folder = os.path.abspath(binary_folder)\n",
        "\n",
        "def megadown(download_link, filename='.', verbose=False):\n",
        "    \"\"\"Use megatools binary executable to download files and folders from MEGA.nz .\"\"\"\n",
        "    filename = ' --path \"'+os.path.abspath(filename)+'\"' if filename else \"\"\n",
        "    wd_old = os.getcwd()\n",
        "    os.chdir(binary_folder)\n",
        "    try:\n",
        "        if platform == \"linux\" or platform == \"linux2\":\n",
        "            subprocess.call(f'./megatools dl{filename}{\" --debug http\" if verbose else \"\"} {download_link}', shell=True)\n",
        "        elif platform == \"win32\":\n",
        "            subprocess.call(f'megatools.exe dl{filename}{\" --debug http\" if verbose else \"\"} {download_link}', shell=True)\n",
        "    except:\n",
        "        os.chdir(wd_old) # don't let user stop download without going back to correct directory first\n",
        "        raise\n",
        "    os.chdir(wd_old)\n",
        "    return filename\n",
        "\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "from os.path import exists\n",
        "\n",
        "def request_url_with_progress_bar(url, filename):\n",
        "    class DownloadProgressBar(tqdm):\n",
        "        def update_to(self, b=1, bsize=1, tsize=None):\n",
        "            if tsize is not None:\n",
        "                self.total = tsize\n",
        "            self.update(b * bsize - self.n)\n",
        "    \n",
        "    def download_url(url, filename):\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                                 miniters=1, desc=url.split('/')[-1]) as t:\n",
        "            filename, headers = urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n",
        "            print(\"Downloaded to \"+filename)\n",
        "    download_url(url, filename)\n",
        "\n",
        "\n",
        "def download(urls, dataset='', filenames=None, force_dl=False, username='', password='', auth_needed=False):\n",
        "    assert filenames is None or len(urls) == len(filenames), f\"number of urls does not match filenames. Expected {len(filenames)} urls, containing the files listed below.\\n{filenames}\"\n",
        "    assert not auth_needed or (len(username) and len(password)), f\"username and password needed for {dataset} Dataset\"\n",
        "    if filenames is None:\n",
        "        filenames = [None,]*len(urls)\n",
        "    for i, (url, filename) in enumerate(zip(urls, filenames)):\n",
        "        print(f\"Downloading File from {url}\")\n",
        "        #if filename is None:\n",
        "        #    filename = url.split(\"/\")[-1]\n",
        "        if filename and (not force_dl) and exists(filename):\n",
        "            print(f\"{filename} Already Exists, Skipping.\")\n",
        "            continue\n",
        "        if 'drive.google.com' in url:\n",
        "            assert 'https://drive.google.com/uc?id=' in url, 'Google Drive links should follow the format \"https://drive.google.com/uc?id=1eQAnaoDBGQZldPVk-nzgYzRbcPSmnpv6\".\\nWhere id=XXXXXXXXXXXXXXXXX is the Google Drive Share ID.'\n",
        "            gdown.download(url, filename, quiet=False)\n",
        "        elif 'mega.nz' in url:\n",
        "            megadown(url, filename)\n",
        "        else:\n",
        "            #urllib.request.urlretrieve(url, filename=filename) # no progress bar\n",
        "            request_url_with_progress_bar(url, filename) # with progress bar\n",
        "\n",
        "import huggingface_hub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class HFModels:\n",
        "    def __init__(self, repo = \"therealvul/so-vits-svc-4.0\", \n",
        "            model_dir = \"hf_vul_models\"):\n",
        "        self.model_repo = huggingface_hub.Repository(local_dir=model_dir,\n",
        "            clone_from=repo, skip_lfs_files=True)\n",
        "        self.repo = repo\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.model_folders = os.listdir(model_dir)\n",
        "        self.model_folders.remove('.git')\n",
        "        self.model_folders.remove('.gitattributes')\n",
        "\n",
        "    def list_models(self):\n",
        "        return self.model_folders\n",
        "\n",
        "    # Downloads model;\n",
        "    # copies config to target_dir and moves model to target_dir\n",
        "    def download_model(self, model_name, target_dir):\n",
        "        if not model_name in self.model_folders:\n",
        "            raise Exception(model_name + \" not found\")\n",
        "        model_dir = self.model_dir\n",
        "        charpath = os.path.join(model_dir,model_name)\n",
        "\n",
        "        gen_pt = next(x for x in os.listdir(charpath) if x.startswith(\"G_\"))\n",
        "        cfg = next(x for x in os.listdir(charpath) if x.endswith(\"json\"))\n",
        "        try:\n",
        "          clust = next(x for x in os.listdir(charpath) if x.endswith(\"pt\"))\n",
        "        except StopIteration as e:\n",
        "          print(\"Note - no cluster model for \"+model_name)\n",
        "          clust = None\n",
        "\n",
        "        if not os.path.exists(target_dir):\n",
        "            os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        gen_dir = huggingface_hub.hf_hub_download(repo_id = self.repo,\n",
        "            filename = model_name + \"/\" + gen_pt) # this is a symlink\n",
        "        \n",
        "        if clust is not None:\n",
        "          clust_dir = huggingface_hub.hf_hub_download(repo_id = self.repo,\n",
        "              filename = model_name + \"/\" + clust) # this is a symlink\n",
        "          shutil.move(os.path.realpath(clust_dir), os.path.join(target_dir, clust))\n",
        "          clust_out = os.path.join(target_dir, clust)\n",
        "        else:\n",
        "          clust_out = None\n",
        "\n",
        "        shutil.copy(os.path.join(charpath,cfg),os.path.join(target_dir, cfg))\n",
        "        shutil.move(os.path.realpath(gen_dir), os.path.join(target_dir, gen_pt))\n",
        "\n",
        "        return {\"config_path\": os.path.join(target_dir,cfg),\n",
        "            \"generator_path\": os.path.join(target_dir,gen_pt),\n",
        "            \"cluster_path\": clust_out}\n",
        "\n",
        "# Example usage\n",
        "# vul_models = HFModels()\n",
        "# print(vul_models.list_models())\n",
        "# print(\"Applejack (singing)\" in vul_models.list_models())\n",
        "# vul_models.download_model(\"Applejack (singing)\",\"models/Applejack (singing)\")\n",
        "\n",
        "    print(\"Finished!\")"
      ],
      "metadata": {
        "id": "K7aE5WZqdStA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download ContentVec (just run this once)\n",
        "os.chdir('/content/so-vits-svc') # force working-directory to so-vits-svc - this line is just for safety and is probably not required\n",
        "download([\"https://huggingface.co/therealvul/so-vits-svc-4.0-init/resolve/main/checkpoint_best_legacy_500.pt\"], filenames=[\"hubert/checkpoint_best_legacy_500.pt\"])"
      ],
      "metadata": {
        "id": "JntsKHUGr9kD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup HF Downloads (run cell and click buttons to download models)\n",
        "#@markdown This ai was created by BRONIES 🙏🙏 which is why all this stuff is down here idk how to remove it but if youre a brony its cool for you i guess\n",
        "from ipywidgets import widgets\n",
        "vul_models = HFModels()\n",
        "\n",
        "def button_eventhandler(but):\n",
        "  vul_models.download_model(but.description, \"models/\"+but.description)\n",
        "\n",
        "for model in vul_models.list_models():\n",
        "  btn = widgets.Button(description=model)\n",
        "  btn.on_click(button_eventhandler)\n",
        "  display(btn)"
      ],
      "metadata": {
        "id": "Ist9AYsybTc7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Other Downloads (.zip) Step o.1\n",
        "#@markdown Please note that 3.0 models are incompatible with 4.0.\n",
        "\n",
        "#@markdown Supported URL types: \n",
        "#@markdown * Google Drive zip\n",
        "#@markdown * MEGA zip\n",
        "#@markdown * Direct zip (+HuggingFace /resolve/ link)\n",
        "\n",
        "#@markdown Example URLs:\n",
        "#@markdown * Kanye: https://mega.nz/file/Dr40kCQI#G3bEWPvUvTa9SBJKQt7rETgcFds4ssnJF0nGN9aAXTk\n",
        "#@markdown * Kendrick: https://mega.nz/file/WmBzgSZa#UD-SFhHBv3aw0obTHW2lGc5yeaMnK8qtKU3OjDKMVKk\n",
        "#@markdown * Carti (v3): https://mega.nz/file/jnwzEJ4K#erlpUaNQ3VyQIIVaQDYge3Kv4pZtyNfQBWA6hUy6uu8\n",
        "#@markdown * Drake: https://mega.nz/file/Sm53wAwI#4PmIrSWDrEP1-pnZb5MJpTcfoHy3OBhBOhn2FVxfyb8\n",
        "#@markdown * Juice Wrld: https://mega.nz/file/5w9kGSJA#MQEQi7lBBBJMBa_rQ5mfGtDXnv96-XhhsNx-xc81ta8\n",
        "#@markdown * Travis: https://mega.nz/file/q652kCZb#VS9IE0Vr3A3PbynDvmkfantFmz_Iik9i3M9DMWeShoE\n",
        "#@markdown * Tyler: https://mega.nz/file/rz5SBBIK#KAhHX8tR-f5yf_aR4dRwF-oE90LpliA4E8v1YFC7ONQ\n",
        "#@markdown * AI Hub: https://discord.gg/Aktyxz4jwA\n",
        "\n",
        "\n",
        "import re\n",
        "model_url = \"https://mega.nz/file/5w9kGSJA#MQEQi7lBBBJMBa_rQ5mfGtDXnv96-XhhsNx-xc81ta8\" #@param {\"type\": \"string\"}\n",
        "if \"huggingface.co\" in model_url.lower():\n",
        "  download([re.sub(r\"/blob/\",\"/resolve/\",model_url)], \n",
        "           filenames=[os.path.join(os.getcwd(),model_url.split(\"/\")[-1])])\n",
        "else:\n",
        "  download([model_url])"
      ],
      "metadata": {
        "id": "oFr2MWaQfR6X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract .zip Downloads - Step o.2\n",
        "# download speaker model files into 'models' directory\n",
        "#import glob, os, shutil\n",
        "#from pathlib import Path\n",
        "#os.makedirs('models', exist_ok=True)\n",
        "#model_zip_paths = glob.glob('/content/**/*.zip', recursive=True)\n",
        "#for model_zip_path in model_zip_paths:\n",
        "#    extract(model_zip_path) # extract inplace\n",
        "#    ckpts_inplace = glob.glob('./*.pth')\n",
        "#    json_inplace = glob.glob('./*.json')\n",
        "#    if os.path.exists(os.path.splitext(model_zip_path)[0]):\n",
        "#      shutil.move(os.path.splitext(model_zip_path)[0],'models')\n",
        "#    elif len(ckpts_inplace):\n",
        "#      for f in ckpts_inplace:\n",
        "#        os.makedirs('models/'+Path(model_zip_path).stem, \n",
        "#                      exist_ok=True)\n",
        "#        shutil.move(f,'models/'+Path(model_zip_path).stem)\n",
        "#      for f in json_inplace:\n",
        "#        shutil.move(f,'models/'+Path(model_zip_path).stem)\n",
        "        #@title Extract .zip Downloads - Step o.2\n",
        "# download speaker model files into 'models' directory\n",
        "import glob, os, shutil\n",
        "from pathlib import Path\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_zip_paths = glob.glob('/content/**/*.zip', recursive=True)\n",
        "\n",
        "for model_zip_path in model_zip_paths:\n",
        "    print(\"extracting zip\",model_zip_path)\n",
        "    output_dir = os.path.join('/content/so-vits-svc/models',os.path.basename(os.path.splitext(model_zip_path)[0]).replace(\" \",\"_\"))\n",
        "    \n",
        "    # clean and create output dir\n",
        "    if os.path.exists(output_dir):\n",
        "      shutil.rmtree(output_dir)\n",
        "    os.mkdir(output_dir)\n",
        "    input_base = os.path.dirname(model_zip_path)\n",
        "\n",
        "    # clean input dir (if user stopped an earlier extract and we have dirty files)\n",
        "    ckpts_pre = glob.glob(os.path.join(input_base,'**/*.pth'),recursive=True)\n",
        "    jsons_pre = glob.glob(os.path.join(input_base,'**/config.json'),recursive=True)\n",
        "    for cpkt in ckpts_pre:\n",
        "      os.remove(cpkt)\n",
        "    for json in jsons_pre:\n",
        "      os.remove(json)\n",
        "\n",
        "    # do the extract\n",
        "    extract(model_zip_path)\n",
        "    ckpts = glob.glob(os.path.join(input_base,'**/*.pth'),recursive=True)\n",
        "    jsons = glob.glob(os.path.join(input_base,'**/config.json'),recursive=True)\n",
        "    for ckpt in ckpts:\n",
        "      shutil.move(ckpt,os.path.join(output_dir,os.path.basename(ckpt)))\n",
        "    for json in jsons:\n",
        "      shutil.move(json,os.path.join(output_dir,os.path.basename(json)))\n"
      ],
      "metadata": {
        "id": "7FHg8Gx8ihDk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Open the file explorer on the left of your screen and drag-and-drop an audio file anywhere. Then run the below cell.\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import copy\n",
        "import logging\n",
        "import io\n",
        "from ipywidgets import widgets\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "os.chdir('/content/so-vits-svc')\n",
        "\n",
        "import torch\n",
        "from inference import infer_tool\n",
        "from inference import slicer\n",
        "from inference.infer_tool import Svc\n",
        "import soundfile\n",
        "import numpy as np\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "def get_speakers():\n",
        "  speakers = []\n",
        "  for _,dirs,_ in os.walk(MODELS_DIR):\n",
        "    for folder in dirs:\n",
        "      cur_speaker = {}\n",
        "      # Look for G_****.pth\n",
        "      g = glob.glob(os.path.join(MODELS_DIR,folder,'G_*.pth'))\n",
        "      if not len(g):\n",
        "        print(\"Skipping \"+folder+\", no G_*.pth\")\n",
        "        continue\n",
        "      cur_speaker[\"model_path\"] = g[0]\n",
        "      cur_speaker[\"model_folder\"] = folder\n",
        "\n",
        "      # Look for *.pt (clustering model)\n",
        "      clst = glob.glob(os.path.join(MODELS_DIR,folder,'*.pt'))\n",
        "      if not len(clst):\n",
        "        print(\"Note: No clustering model found for \"+folder)\n",
        "        cur_speaker[\"cluster_path\"] = \"\"\n",
        "      else:\n",
        "        cur_speaker[\"cluster_path\"] = clst[0]\n",
        "\n",
        "      # Look for config.json\n",
        "      cfg = glob.glob(os.path.join(MODELS_DIR,folder,'*.json'))\n",
        "      if not len(cfg):\n",
        "        print(\"Skipping \"+folder+\", no config json\")\n",
        "        continue\n",
        "      cur_speaker[\"cfg_path\"] = cfg[0]\n",
        "      with open(cur_speaker[\"cfg_path\"]) as f:\n",
        "        try:\n",
        "          cfg_json = json.loads(f.read())\n",
        "        except Exception as e:\n",
        "          print(\"Malformed config json in \"+folder)\n",
        "        for name, i in cfg_json[\"spk\"].items():\n",
        "          cur_speaker[\"name\"] = name\n",
        "          cur_speaker[\"id\"] = i\n",
        "          if not name.startswith('.'):\n",
        "            speakers.append(copy.copy(cur_speaker))\n",
        "\n",
        "    return sorted(speakers, key=lambda x:x[\"name\"].lower())\n",
        "\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "chunks_dict = infer_tool.read_temp(\"inference/chunks_temp.json\")\n",
        "existing_files = []\n",
        "slice_db = -40\n",
        "wav_format = 'wav'\n",
        "\n",
        "class InferenceGui():\n",
        "  def __init__(self):\n",
        "    self.speakers = get_speakers()\n",
        "    self.speaker_list = [x[\"name\"] for x in self.speakers]\n",
        "    self.speaker_box = widgets.Dropdown(\n",
        "        options = self.speaker_list\n",
        "    )\n",
        "    display(self.speaker_box)\n",
        "\n",
        "    def convert_cb(btn):\n",
        "      self.convert()\n",
        "    def clean_cb(btn):\n",
        "      self.clean()\n",
        "\n",
        "    self.convert_btn = widgets.Button(description=\"Convert\")\n",
        "    self.convert_btn.on_click(convert_cb)\n",
        "    self.clean_btn = widgets.Button(description=\"Delete all audio files\")\n",
        "    self.clean_btn.on_click(clean_cb)\n",
        "\n",
        "    self.trans_tx = widgets.IntText(value=0, description='Transpose')\n",
        "    self.cluster_ratio_tx = widgets.FloatText(value=0.0, \n",
        "      description='Clustering Ratio')\n",
        "    self.noise_scale_tx = widgets.FloatText(value=0.4, \n",
        "      description='Noise Scale')\n",
        "    self.auto_pitch_ck = widgets.Checkbox(value=False, description=\n",
        "      'Auto pitch f0 (do not use for singing)')\n",
        "\n",
        "    display(self.trans_tx)\n",
        "    display(self.cluster_ratio_tx)\n",
        "    display(self.noise_scale_tx)\n",
        "    display(self.auto_pitch_ck)\n",
        "    display(self.convert_btn)\n",
        "    display(self.clean_btn)\n",
        "\n",
        "  def convert(self):\n",
        "    trans = int(self.trans_tx.value)\n",
        "    speaker = next(x for x in self.speakers if x[\"name\"] == \n",
        "          self.speaker_box.value)\n",
        "    spkpth2 = os.path.join(os.getcwd(),speaker[\"model_path\"])\n",
        "    print(spkpth2)\n",
        "    print(os.path.exists(spkpth2))\n",
        "\n",
        "    svc_model = Svc(speaker[\"model_path\"], speaker[\"cfg_path\"], \n",
        "      cluster_model_path=speaker[\"cluster_path\"])\n",
        "    \n",
        "    input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "    for name in input_filepaths:\n",
        "      print(\"Converting \"+os.path.split(name)[-1])\n",
        "      infer_tool.format_wav(name)\n",
        "\n",
        "      wav_path = str(Path(name).with_suffix('.wav'))\n",
        "      wav_name = Path(name).stem\n",
        "      chunks = slicer.cut(wav_path, db_thresh=slice_db)\n",
        "      audio_data, audio_sr = slicer.chunks2audio(wav_path, chunks)\n",
        "\n",
        "      audio = []\n",
        "      for (slice_tag, data) in audio_data:\n",
        "          print(f'#=====segment start, '\n",
        "              f'{round(len(data)/audio_sr, 3)}s======')\n",
        "          \n",
        "          length = int(np.ceil(len(data) / audio_sr *\n",
        "              svc_model.target_sample))\n",
        "          \n",
        "          if slice_tag:\n",
        "              print('jump empty segment')\n",
        "              _audio = np.zeros(length)\n",
        "          else:\n",
        "              # Padding \"fix\" for noise\n",
        "              pad_len = int(audio_sr * 0.5)\n",
        "              data = np.concatenate([np.zeros([pad_len]),\n",
        "                  data, np.zeros([pad_len])])\n",
        "              raw_path = io.BytesIO()\n",
        "              soundfile.write(raw_path, data, audio_sr, format=\"wav\")\n",
        "              raw_path.seek(0)\n",
        "              _cluster_ratio = 0.0\n",
        "              if speaker[\"cluster_path\"] != \"\":\n",
        "                _cluster_ratio = float(self.cluster_ratio_tx.value)\n",
        "              out_audio, out_sr = svc_model.infer(\n",
        "                  speaker[\"name\"], trans, raw_path,\n",
        "                  cluster_infer_ratio = _cluster_ratio,\n",
        "                  auto_predict_f0 = bool(self.auto_pitch_ck.value),\n",
        "                  noice_scale = float(self.noise_scale_tx.value))\n",
        "              _audio = out_audio.cpu().numpy()\n",
        "              pad_len = int(svc_model.target_sample * 0.5)\n",
        "              _audio = _audio[pad_len:-pad_len]\n",
        "          audio.extend(list(infer_tool.pad_array(_audio, length)))\n",
        "          \n",
        "      res_path = os.path.join('/content/',\n",
        "          f'{wav_name}_{trans}_key_'\n",
        "          f'{speaker[\"name\"]}.{wav_format}')\n",
        "      soundfile.write(res_path, audio, svc_model.target_sample,\n",
        "          format=wav_format)\n",
        "      display(Audio(res_path, autoplay=True)) # display audio file\n",
        "    pass\n",
        "\n",
        "  def clean(self):\n",
        "     input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "     for f in input_filepaths:\n",
        "       os.remove(f)\n",
        "\n",
        "inference_gui = InferenceGui()"
      ],
      "metadata": {
        "id": "Bpg6Ql1QHEV6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}